# To install MVPA2, build from sources section in http://www.pymvpa.org/installation.html
# 
# [TODO, ABOVE IS NON TRIVIAL]
# 

#from mvpa2.suite import *
import mvpa2
"""
First, we define some colors as RGB values from the interval (0,1), i.e. with white being (1, 1, 1) 
and black being (0, 0, 0). Please note, that a substantial proportion of the defined colors represent 
variations of ‘blue’, which are supposed to be represented in more detail in the SOM.

"""
colors = np.array(
         [[0., 0., 0.],
          [0., 0., 1.],
          [0., 0., 0.5],
          [0.125, 0.529, 1.0],
          [0.33, 0.4, 0.67],
          [0.6, 0.5, 1.0],
          [0., 1., 0.],
          [1., 0., 0.],
          [0., 1., 1.],
          [1., 0., 1.],
          [1., 1., 0.],
          [1., 1., 1.],
          [.33, .33, .33],
          [.5, .5, .5],
          [.66, .66, .66]])

# store the names of the colors for visualization later on
color_names =         ['black', 'blue', 'darkblue', 'skyblue',
         'greyblue', 'lilac', 'green', 'red',
         'cyan', 'violet', 'yellow', 'white',
         'darkgrey', 'mediumgrey', 'lightgrey']

# Now we can instantiate the mapper. It will internally use a so-called Kohonen layer to map the data onto. 
# We tell the mapper to use a rectangular layer with 20 x 30 units. This will be the output space of the mapper. 
# Additionally, we tell it to train the network using 400 iterations and to use custom learning rate.
som = SimpleSOMMapper((20, 30), 400, learning_rate=0.05)
#Finally, we train the mapper with the previously defined ‘color’ dataset.

som.train(colors)
#Each unit in the Kohonen layer can be treated as a pointer into the high-dimensional input space, that can be queried
# to inspect which input subspaces the SOM maps onto certain sections of its 2D output space. The color-mapping generated by this example’s SOM can be shown with a single matplotlib call:

pl.imshow(som.K, origin='lower')
# And now, let’s take a look onto which coordinates the initial training prototypes were mapped to. 
# The get those coordinates we can simply feed the training data to the mapper and plot the output.

mapped = som(colors)

pl.title('Color SOM')
# SOM's kshape is (rows x columns), while matplotlib wants (X x Y)
for i, m in enumerate(mapped):
    pl.text(m[1], m[0], color_names[i], ha='center', va='center',
           bbox=dict(facecolor='white', alpha=0.5, lw=0))





# CREDIT: Tutorial from: https://github.com/sevamoo/SOMPY
# 

# USING SOMPY: https://gist.github.com/sevamoo/035c56e7428318dd3065013625f12a11

import random
import numpy as np
import pandas as pd

import matplotlib.pylab as plt
get_ipython().magic('matplotlib inline')
# import sompy as sompy
import pandas as pd
import numpy as np
from time import time
import sompy
### A toy example: two dimensional data, four clusters

dlen = 200
Data1 = pd.DataFrame(data= 1*np.random.rand(dlen,2))
Data1.values[:,1] = (Data1.values[:,0][:,np.newaxis] + .42*np.random.rand(dlen,1))[:,0]


Data2 = pd.DataFrame(data= 1*np.random.rand(dlen,2)+1)
Data2.values[:,1] = (-1*Data2.values[:,0][:,np.newaxis] + .62*np.random.rand(dlen,1))[:,0]

Data3 = pd.DataFrame(data= 1*np.random.rand(dlen,2)+2)
Data3.values[:,1] = (.5*Data3.values[:,0][:,np.newaxis] + 1*np.random.rand(dlen,1))[:,0]


Data4 = pd.DataFrame(data= 1*np.random.rand(dlen,2)+3.5)
Data4.values[:,1] = (-.1*Data4.values[:,0][:,np.newaxis] + .5*np.random.rand(dlen,1))[:,0]


Data1 = np.concatenate((Data1,Data2,Data3,Data4))

fig = plt.figure()
plt.plot(Data1[:,0],Data1[:,1],'ob',alpha=0.2, markersize=4)
fig.set_size_inches(7,7)


mapsize = [20,20]
som = sompy.SOMFactory.build(Data1, mapsize, mask=None, mapshape='planar', lattice='rect', normalization='var', initialization='pca', neighborhood='gaussian', training='batch', name='sompy')  # this will use the default parameters, but i can change the initialization and neighborhood methods
som.train(n_job=1, verbose='info')  # verbose='debug' will print more, and verbose=None wont print anything


v = sompy.mapview.View2DPacked(50, 50, 'test',text_size=8)  
# could be done in a one-liner: sompy.mapview.View2DPacked(300, 300, 'test').show(som)
v.show(som, what='codebook', which_dim=[0,1], cmap=None, col_sz=6) #which_dim='all' default
# v.save('2d_packed_test')


som.component_names = ['1','2']
v = sompy.mapview.View2DPacked(50, 50, 'test',text_size=8)  
v.show(som, what='codebook', which_dim='all', cmap='jet', col_sz=6) #which_dim='all' default


# c = sompy.mapview.View2DPacked()
v = sompy.mapview.View2DPacked(2, 2, 'test',text_size=8)  
#first you can do clustering. Currently only K-means on top of the trained som
cl = som.cluster(n_clusters=4)
# print cl
getattr(som, 'cluster_labels')


v.show(som, what='cluster')


h = sompy.hitmap.HitMapView(10, 10, 'hitmap', text_size=8, show_text=True)
h.show(som)


u = sompy.umatrix.UMatrixView(50, 50, 'umatrix', show_axis=True, text_size=8, show_text=True)

#This is the Umat value
UMAT  = u.build_u_matrix(som, distance=1, row_normalized=False)

#Here you have Umatrix plus its render
UMAT = u.show(som, distance2=1, row_normalized=False, show_data=True, contooor=True, blob=False)


dlen = 700
tetha = np.random.uniform(low=0,high=2*np.pi,size=dlen)[:,np.newaxis]
X1 = 3*np.cos(tetha)+ .22*np.random.rand(dlen,1)
Y1 = 3*np.sin(tetha)+ .22*np.random.rand(dlen,1)
Data1 = np.concatenate((X1,Y1),axis=1)

X2 = 1*np.cos(tetha)+ .22*np.random.rand(dlen,1)
Y2 = 1*np.sin(tetha)+ .22*np.random.rand(dlen,1)
Data2 = np.concatenate((X2,Y2),axis=1)

X3 = 5*np.cos(tetha)+ .22*np.random.rand(dlen,1)
Y3 = 5*np.sin(tetha)+ .22*np.random.rand(dlen,1)
Data3 = np.concatenate((X3,Y3),axis=1)

X4 = 8*np.cos(tetha)+ .22*np.random.rand(dlen,1)
Y4 = 8*np.sin(tetha)+ .22*np.random.rand(dlen,1)
Data4 = np.concatenate((X4,Y4),axis=1)



Data2 = np.concatenate((Data1,Data2,Data3,Data4),axis=0)

fig = plt.figure()
plt.plot(Data2[:,0],Data2[:,1],'ob',alpha=0.2, markersize=4)
fig.set_size_inches(7,7)
# plt.plot(np.cos(tetha))


mapsize = [30,30]
som = sompy.SOMFactory.build(Data2, mapsize, mask=None, mapshape='planar', lattice='rect', normalization='var', initialization='pca', neighborhood='gaussian', training='batch', name='sompy')  # this will use the default parameters, but i can change the initialization and neighborhood methods
som.train(n_job=1, verbose='info')  # verbose='debug' will print more, and verbose=None wont print anything
mapsize = [30,30]
som = sompy.SOMFactory.build(Data2, mapsize, mask=None, mapshape='planar', lattice='rect', normalization='var', initialization='pca', neighborhood='gaussian', training='batch', name='sompy')  # this will use the default parameters, but i can change the initialization and neighborhood methods
som.train(n_job=1, verbose='info')  # verbose='debug' will print more, and verbose=None wont print anything


v = sompy.mapview.View2DPacked(50, 50, 'test',text_size=8)  
# could be done in a one-liner: sompy.mapview.View2DPacked(300, 300, 'test').show(som)
v.show(som, what='codebook', which_dim=[0,1], cmap=None, col_sz=6) #which_dim='all' default
# v.save('2d_packed_test')


#In this case, K-means simply fails as expected
v = sompy.mapview.View2DPacked(2, 2, 'test',text_size=8)  
#first you can do clustering. Currently only K-means on top of the trained som
cl = som.cluster(n_clusters=4)
v.show(som, what='cluster')
# v.save('kmeans_test')


#But Umatrix finds the clusters easily
u = sompy.umatrix.UMatrixView(50, 50, 'umatrix', show_axis=True, text_size=8, show_text=True)

#This is the Umat value
UMAT  = u.build_u_matrix(som, distance=1, row_normalized=False)

UMAT = u.show(som, distance2=1, row_normalized=False, show_data=True, contooor=False, blob=False)





# ## Descriptive statistics
# 
# Assuming some data from a MOOC platform's AB Test, this notebook performs dynamic exploration of dimensions and some 
# descriptive statistics for the dimension of 'Gender' and per revenue per session that a user logs into the system:
# 
# rps_diff_frac = fractional differential lift of the revenue with respect to treatment subset of female variant
# 
# rps_diff = absolute differential lift of the revenue
# 
# rps_ctrl = mean for the revenue for the control variant of the experiment
# 
# rps_diff_err = Std error of the mean (SEM)
# 
# * Note: Standard Error of the Mean (SEM) σM = the stdev of the sampling distribution of the mean, where σ is the standard deviation of the original distribution and N is the sample size (the number of scores each mean is based upon).
# 
# Two ways of computing it: standard deviation and standard error of the mean:
# 
# a) s = pd.Series(np.random.randn(1000))
# 
# stats.sem(s.values) # stats.sem(s, axis=None, ddof=0) # n degrees of freedom
# 
# b) s.std() / np.sqrt(len(s))
# 
# 
# ----
# Output:
# 
# rps_diff_frac = 1.062
# 
# rps_diff = 0.54
# 
# rps_ctrl = 8.71
# 
# rps_diff_err = 0.059
# 

import pandas as pd
import numpy as np
from scipy import stats

#from qtextasdata import QTextAsData,QInputParams
# def query_database_harelba_q():
#     # Create an instance of q. Default input parameters can be provided here if needed
#     q = QTextAsData()

#     # execute a query, using specific input parameters
#     r = q.execute('select * from /etc/passwd',QInputParams(delimiter=':'))

#     # Get the result status (ok/error). In case of error, r.error will contain a QError instance with the error information
#     print r.status

sessions = pd.read_csv('./data/sessions-hypercube.csv')
sessions_orig = pd.read_csv('./data/sessions-with-features.csv')
orig_sessions_female_control = sessions_orig.loc[sessions_orig['gender'] == 'female'].loc[sessions_orig['variant']== 'control']
print sessions_orig.head(), len(sessions_orig)
print orig_sessions_female_control.head(), len(orig_sessions_female_control)

#female_sessions = sessions.loc[sessions['gender'] == 'female']
female_sessions_control = sessions.loc[sessions['gender'] == 'female'].loc[sessions['variant']== 'control']
female_sessions_test = sessions.loc[sessions['gender'] == 'female'].loc[sessions['variant']== 'test']
#print female_sessions_control.head(), len(female_sessions_control)

rps_female_ctrl = np.divide(female_sessions_control.rps_sum, female_sessions_control.n)
type(pd.DataFrame(rps_female_ctrl))
rps_female_ctrl = np.divide(female_sessions_control.rps_sum, female_sessions_control.n)
type(pd.DataFrame(rps_female_ctrl))

female_sessions_control['mean_rps'] = female_sessions_control.rps_sum/female_sessions_control.n 
female_sessions_test['mean_rps'] = female_sessions_test.rps_sum/female_sessions_test.n 

print female_sessions_control.head(), type(female_sessions_control)

print "\nrps_ctrl: ", female_sessions_control['mean_rps'].mean()
print "rps_test: ",female_sessions_test['mean_rps'].mean()

print "rps_diff: ", np.abs(female_sessions_control['mean_rps'].mean() 
                           - female_sessions_test['mean_rps'].mean())

print "rps_diff_frac: ", np.divide(female_sessions_test['mean_rps'].mean(),
                        female_sessions_control['mean_rps'].mean())

print "rps_diff_err: ", stats.sem(orig_sessions_female_control['rps'])






# We are given some hints regarding the locations of a person. The following point of interest hints aim at locating the most probable GPS coordinates and the distribution over the areas where he may be easier found.
# 

import pandas as pd
import numpy as np
get_ipython().magic('matplotlib inline')
from matplotlib import pyplot as plt
import math
from matplotlib.colors import LogNorm  # numpy.random.lognormal(mean=0.0, sigma=1.0, size=None)
from numpy.random import randn
from scipy.stats import lognorm
import pylab as pl
get_ipython().magic('pylab inline')
    
def shortest_distance_to_river(x,y, riverDF):
    # returns shortest distance to the river
    # river contains the coordinates (x,y) of the river
    riverDF['distance'] = np.sqrt((riverDF['x']-x)**2 + (riverDF['y']-y)**2)
    min_dist = riverDF['distance'].min()
    #riverDF['distance'] = ((riverDF['x']-x)**2 + (riverDF['y']-y)**2).apply(np.sqrt)
    print riverDF.head(),'\n', " Shortest distance to the river: ",min_dist
    return min_dist

def cartesian2polar(theta):
    # cartesian coordinates to polar ones.
    # theta goes from 0 to 2pi:     theta = np.linspace(0, 2*np.pi, 100)
    x1 = r*cos(theta)
    x2 = r*sin(theta)
    # if you use these substitions in the circle equation you will see that r=sqrt(0.6).
    return x1, x2

def GPS2XYcoords(SW_lat, SW_lon, P_lat, P_lon):
    # The x and y coordinates of a GPS coordinate P with (P_lat, P_lon) can be calculated using: 
    Px = (P_lon - SW_lon) * np.cos(SW_lat * np.pi / 180) *111.323 
    Py = (P_lat - SW_lat) * 111.323
    #print "X,Y coords: ", Px, Py
    return Px, Py

def latitude2XCoord(P_lat):
    # South-west corner of the area we are interested in:  
    SW_lat = 52.464011 # (Latitude) 
    return (P_lat - SW_lat) * 111.323
    
def longitude2YCoord(P_lon):
    # South-west corner of the area we are interested in: 
    SW_lat = 52.464011 # (Latitude) 
    SW_lon = 13.274099 #(Longitude)
    return (P_lon - SW_lon) * np.cos(SW_lat * np.pi / 180) *111.323 

def GPS2XYcoordsDF(SW_lat, SW_lon, P_lat, P_lon):
    Px, Py = GPS2XYcoords(SW_lat, SW_lon, P_lat, P_lon)
    return pd.DataFrame({'x':[Px], 'y':[Py]})

def interpolate1D(x, y, title):
    from scipy.interpolate import interp1d
    fig = plt.figure()
    fig.suptitle(title)
    f = interp1d(x, y)
    #f2 = interp1d(x, y, kind='cubic')
    xnew = np.linspace(x.min(), x.max(),num=len(x), endpoint=True) 
    plt.plot(x, y, 'o', xnew, f(xnew), '-')#, xnew, f2(xnew), '--')
    plt.legend(['data', 'linear', 'cubic'], loc='best')
    plt.show()
    fig.savefig(title.join('.png'))

def gaussian(x_range_left, x_range_right, mu, sigma):
    """
     A*np.exp(-(x-mu)**2/(2.*sigma**2))
    """
    from scipy.stats import norm
    # http://yaboolog.blogspot.com.es/2011/07/python-tips-draw-gaussian-pdf-graph.html
    # Plot between -10 and 10 with .001 steps.
    print "Plotting Gaussian for x in range ", x_range_left, x_range_right
    #fig = plt.figure()
    x_axis = np.arange(x_range_left, x_range_right, 0.001)
    gaussian = norm.pdf(x_axis, mu, sigma)
    fig = plt.plot(x_axis, gaussian) # pdf(axis_range, mu, sigma)
    # TODO: how to save this plot? fig.savefig('Gaussian-funct-of-candidates-shortest-distance-to-river.png')
    plt.show() 
    return x_axis, gaussian   
    
def gaussian_of_shortest_dist_to_river(x, y, riverGPS_df):
    """
    The candidate is likely to be close to the river Spree. The probability at any point is given
    by a Gaussian function of its shortest distance to the river. The function peaks at zero and
    has 95% of its total integral within +/-2730m.
    "Peaks at zero" -> the Gaussian has as mean = 0?
    "a Gaussian function of its shortest distance to the river"-> the Gaussian has as mean the shortest distance?
    SE = stdev(x)/sqrt(N)
    If the underlying distribution of the coefficients is normal, the 95% confidence interval is 
    [mean-2*sigma,mean+2*sigma], so the standard deviation is 1/4 the width of the interval. See [1]
    """
    d = shortest_distance_to_river(x,y,riverGPS_df)
    sigma = 2730/2
    return gaussian(riverGPS_df.x.min(), riverGPS_df.x.max(), d, sigma) # TODO: d or 0?
             
def draw_River_Spree():
    GPS = pd.read_csv('./data/RiverSpreeGPS.csv', header=None) 
    GPS.columns = ['x','y']
    #print "GPS before\n", GPS.head()
    GPS['x'] = GPS['x'].apply(latitude2XCoord)
    GPS['y'] = GPS['y'].apply(longitude2YCoord)
    interpolate1Ds(GPS['x'], GPS['y'], GPS['x']-2730, GPS['y']-2730, GPS['x']+2730, GPS['y']+2730,"River Spree and Gaussian function of the shortest distance to the river", 'cubic')
    gaussian_of_shortest_dist_to_river(0, 0, GPS)
    #GPS.plot('x', 'y', kind='scatter') 
    return GPS

def draw_log_normal_pdf_around_Brandenburg_gate(SW_lat, SW_lon):
    """
    A probability distribution centered around the Brandenburg Gate also informs us of the
    candidate’s location. The distribution’s radial profile is log-normal with a mean of
    4700m and a mode of 3877m in every direction. 
    Brandenburg Gate GPS coordinates
    52.516288,13.377689

    On a logarithmic scale, mu and sigma can be called the location parameter and the scale parameter, 
    respectively in a LogNorm. The mode is the point of global maximum of the probability density function; 
    It solves the equation (ln f)'=0
    The probability density function of the lognormal distribution with parameters μ and σ, f, increases 
    and then decreases with mode at x = exp(μ−σ^2) 
    REF: http://www.math.uah.edu/stat/special/LogNormal.html
    {Mode}[X]=e^{mu -sigma^{2}}
    TODO: "Centered around the Gate" -> the gate's x coord is its mean?
        # e.g. normal distribution center at x=0 and y=5:
        # x = randn(100000)     #     y = randn(100000)+5
    TODO Alternative: use colormap normalization: http://matplotlib.org/users/colormapnorms.html
    """    
    # the coordinates of the gate are the center of our PDF of the LogNormal
    center_x, center_y = GPS2XYcoords(SW_lat, SW_lon, 52.516288, 13.377689) #7.0259513879, 5.819632471 #GPS2XYcoords(SW_lat, SW_lon, 52.516288, 13.377689)
    mu = 4700 # mean
    mode = 3877   
    #mode = np.exp(mu - sigma^2)  from this expression, we can apply logarithms and obtain stdev as:  
    stddev = mu - np.log(mode)
    print "We obtain stddev from the mean and mode: ", stddev, " and center the gate on (X,Y)= ",center_x, center_y
    dist=lognorm([stddev],loc= center_x)#loc=mu)  # localize around center_x?
    x= np.linspace(5,10, 200)#latitude2XCoord(SW_lat), latitude2XCoord(center_x+8), 200)
    pl.plot(x, dist.pdf(x), '-b', label='LogNorm') # dist.cdf
    pl.legend(loc='upper right')
    #pl.plot(x + center_x, dist.pdf(x + center_x))  # also possible: dist.cdf

    pl.title('LogNorm PDF centered around the Gate (Coord X,Y =%f,  %f)'%(center_x, center_y))
    pl.savefig('LogNorm-centered-around-gate.png')
    print "Radial LogNormal drawn!" 
    #return xedges, yedges

def draw_map():
    """
    Draws all areas of interest in the map
    TODO: 
    1) Use colormap with sentinels: 
    https://scipy.github.io/old-wiki/pages/Cookbook/Matplotlib/Plotting_Images_with_Special_Values.html 
    2) Plot all features in only one plot: Pylab and Matplotlib do not allow it? / Use only matplotlib?
    3) Use proper map with density estimation http://scikit-learn.org/stable/auto_examples/neighbors/plot_species_kde.html#sphx-glr-auto-examples-neighbors-plot-species-kde-py
    """
    fig = plt.figure()
    
    # Draw Gaussian of the shortest distance to the river
    # draw_River_Spree() 
    
    #ax = fig.gca()
    #ax = fig.add_subplot(1, 1, 1)
    #ax.set_xlim(SW_x, SW_y)     #fig, ax = plt.subplots() # note we must use plt.subplots, not plt.subplot
    # (or if you have an existing figure)
    # fig = plt.gcf()
    # ax = fig.gca()
        
    # TODO: problem plotting: pylab and matplotlib do not plot together?
    # Draw Radial LogNormal around Brandenburg Gate
    # draw_log_normal_pdf_around_Brandenburg_gate(SW_lat, SW_lon)   
    # Draw satellite path
    # satellite_path = draw_Satellite_path()   
    
    print "Full map drawn!"  
    plt.show()
    fig.suptitle("Candidate's probable locations for search\n River Spree and Gaussian function of the shortest distance to the river")  
    fig.savefig('map.png')

def interpolate1Ds(x, y, lower_x, lower_y, upper_x, upper_y, title, interpolation_type):
    from scipy.interpolate import interp1d
    """ 
    For more than 1D grids, see https://docs.scipy.org/doc/scipy-0.18.0/reference/tutorial/interpolate.html
    """
    print len(x), "datapoints", x.min(), x.max()
    print len(lower_x), "datapoints", lower_x.min(), lower_x.max()
    print len(upper_x), "datapoints", upper_x.min(), upper_x.max()    
    fig = plt.figure()
    fig.suptitle(title)
    f = interp1d(x, y) 
    f3 = interp1d(x, y, kind=interpolation_type)
    f_low = interp1d(lower_x, lower_y)#, kind='cubic')
    f_upp = interp1d(upper_x, upper_y)#, kind='cubic')
    #f2 = interp1d(x, y, kind='cubic')
    xnew = np.linspace(x.min(), x.max(), num=len(x)*2, endpoint=True) 
    lower_margin_x = np.linspace(lower_x.min(), lower_x.max(), num=len(lower_x)*2, endpoint=True) 
    upper_margin_x = np.linspace(upper_x.min(), upper_x.max(),num=len(upper_x)*2, endpoint=True)
    plt.plot(x, y, 'o', xnew, f(xnew), '-', xnew, f3(xnew), '--')
    # TODO: DOES NOT WORK, WHY DOES IT PLOTS ONLY THE FIRST POINT?
    # a) plt.plot(x, y, 'o', xnew, f3(xnew), '-', lower_margin_x, f_low(lower_margin_x), '--', upper_margin_x, f_upp(upper_margin_x), '+')# xnew, f2(xnew), '--')
    # b) plt.plot(lower_x, lower_y, 'o', lower_margin_x, f_low(lower_margin_x), 'r--')
    #    plt.plot(upper_x, upper_y, 'o', upper_margin_x, f_upp(upper_margin_x), 'g+')
    #plt.legend(['data', 'linear', 'lower', 'low_linear', 'upper', 'up_linear'], loc='best')
    plt.legend(['data', 'linear', interpolation_type], loc='best')
    plt.ylim([y.min(), y.max()])
    plt.xlim([x.min(), x.max()])
    title = title +'.png'
    fig.savefig(title)
#     plt.ylim([-3000, 3000])
#     plt.xlim([-3000, 3000])

def draw_Satellite_path():
    """
    A satellite offers further information: with 95% probability she is located within 2400m
    distance of the satellite’s path (assuming a normal probability distribution).
    Satellite path is a great circle path between coordinates:
    52.590117,13.39915
    52.437385,13.553989
    NOTE: In this case we do not need to plot a circle, the satellite is real and its projection on 
    Earth will be the projected line we need to plot.
    """
    fig = plt.figure()
    x, y = GPS2XYcoords(SW_lat, SW_lon, 52.590117,13.39915)
    x1, y1 = GPS2XYcoords(SW_lat, SW_lon, 52.437385,13.553989)
    sat = pd.DataFrame({'x':[x, x1], 'y':[y, y1]})
    print "Satellite path (X,Y): ", sat.head()
    #does not work: interpolate1Ds(sat.x, sat.y, sat.x+2400, sat.y+2400, sat.x-2400, sat.y-2400,"Normal Probab. Distrib. along the Satellite path", 'linear')
    title = 'Satellite path along (X1,Y1)= (' + str(round(x,2)) +','+str(round(y,2))+') and (X2,Y2)= ('+ str(round(x1,2))+','+ str(round(y1,2))+')'
    plot_gaussian3D(sat.x, sat.y, title)
    plt.show()
    fig.savefig('satellite-path.png')

    
def plot_gaussian3D(x, y, title):
    from matplotlib.mlab import bivariate_normal
    from mpl_toolkits.mplot3d import Axes3D
    #Parameters to set
    mu_x = x.mean()
    sigma_x = x.std()
    mu_y = y.mean()
    sigma_y = y.std()

    #Create grid and multivariate normal
    x = np.linspace(x.min(), x.max(),500)
    y = np.linspace(y.min(),y.max(),500)
    X, Y = np.meshgrid(x,y)
    Z = bivariate_normal(X,Y,sigma_x,sigma_y,mu_x,mu_y)

    #Make a 3D plot
    fig = plt.figure()
    ax = fig.gca(projection='3d')
    ax.plot_surface(X, Y, Z,cmap='viridis',linewidth=0)
    ax.set_xlabel('X axis')
    ax.set_ylabel('Y axis')
    ax.set_zlabel('Z axis')
    fig.suptitle(title)
    plt.show()
    title += '.png'
    fig.savefig(title)

# South-west corner of the area we are interested in: 
SW_lat = 52.464011 # (Latitude) 
SW_lon = 13.274099 #(Longitude)

# Draw Gaussian of the shortest distance to the river
draw_River_Spree() 


# Draw Radial LogNormal around Brandenburg Gate
draw_log_normal_pdf_around_Brandenburg_gate(SW_lat, SW_lon)


# Draw satellite path
draw_Satellite_path()


# # APPENDIX
# 
# 
# ## REFERENCES
# 
# [1] SD = sqrt(N)(upper limit-lower limit)/3.92   
# 
# for a 95% CI if the sample size is large (>100.e.g in each group)
#     http://handbook.cochrane.org/chapter_7/7_7_3_2_obtaining_standard_deviations_from_standard_errors_and.htm
#     TODO: norminv(0.975)*sigma=1.9600*sigma
#         
# 
# [2] Approximating a circle with Bezier curves: http://www.tinaja.com/glib/ellipse4.pdf
# 
# [3] Fitting a circle to a set of points through least squares: http://scipy-cookbook.readthedocs.io/items/Least_Squares_Circle.html
# 
# 
# ## CODE SNIPPETS
# 

def draw_circle_passing_by(x, y, x1, y1):
    """
    If we were to plot a circle, we could do it with 2 or more points with Least Squares Circle: 
    http://scipy-cookbook.readthedocs.io/items/Least_Squares_Circle.html
    https://gist.github.com/lorenzoriano/6799568
    """
    if x>x1:
        center_x = x1+(x-x1)/2
    else:
        center_x = x+ (x1-x)/2
    if y>y1:
        center_y = y1+(y-y1)/2
    else:
        center_y = y+ (y1-y)/2
    radius = math.sqrt((x - center_x)**2 + (y - center_y)**2)
    print "Satellite cicular path drawn!"
    satellite_path = draw_circle(center_x, center_y, radius, 'r')
    outer = draw_circle(center_x, center_y, radius+2400, 'g')
    inner = draw_circle(center_x, center_y, radius-2400, 'b')
    return outer, satellite_path, inner

def draw_circle(x, y, r, color):
    fig, ax = plt.subplots() # note we must use plt.subplots, not plt.subplot
    # Circle takes (x,y) coordinates of the circle's center, and radius r
    circle = plt.Circle((x,y), r, color=color, fill=False, clip_on=False) #  make a circle with no fill
    # Add that artist to an instance of axes:
    ax.add_artist(circle)
    plt.show()
    print "Circle drawn!"
    return circle






from scipy import optimize
def piecewise_linear(x, x0, y0, k1, k2):
    return np.piecewise(x, [x < x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])

def fit_curve(x, y):
    p , e = optimize.curve_fit(piecewise_linear, x, y)
    xd = np.linspace(0, 15, 100)
    plt.plot(x, y, "o")
    plt.plot(xd, piecewise_linear(xd, *p))
    plt.show()
    
x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,11, 12, 13, 14, 15], dtype=float)
y = np.array([5, 7, 9, 11, 13, 15, 28.92, 42.81, 56.7, 70.59, 84.47, 98.36, 112.25, 126.14, 140.03])
fit_curve(x,y)


from matplotlib.colors import LogNorm
import matplotlib.pyplot as plt
from numpy.random import randn

def LogNorm_density():
    #normal distribution center at x=0 and y=5
    x = randn(100000)
    y = randn(100000)+5

    H, xedges, yedges, img = plt.hist2d(x, y, norm=LogNorm())
    fig = plt.figure()
    plt.show()
    
LogNorm_density()





# ## Linear models for heavy tailed data with outliers 
# 
# The first task to understand our data is plotting x, y and see the dispersion, distribution and residuals from our model r = y -yhat. (yhat or y_predicted). If the mean of the residuals is not zero, then there is an underlying process to be studied. 
# We first do regular OLS and see if the coef. of determination R2 (best metric to assess a model in general when having linear models) shows a good fit quality (the closer to 1 the better, i.e., the model explains all the variance in our data). If residuals do not have mean 0 (i.e., the noise is not random), such as in our case, there is a process to study and room for improvement in our model on the outliers.
# 
# 
# A good choice when data like ours has fat tails is Huber Loss function, used in robust regression, that is less sensitive to outliers than the squared error loss. Huber loss function is quadratic for small values of a, and linear for large values (it is defined piecewise). Because the squared loss has the disadvantage that it has the tendency to be dominated by outliers and its mean is poor for heavy-tailed distributions, after the different models tested on OLS and other regularization mthods below, we conclude using Huber loss function, which is used in robust statistics, M-estimation and additive modelling.
# 
# 
# REFERENCES
# 
# 
# <li> http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/glm.html
# 
# 
# <li>Robust regression: http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/robust_models_1.html
# 
# 
# DEPENDENCY LIBRARIES:
# 
# <li>http://minepy.readthedocs.io/en/latest/
# 
# <li>minepy - Maximal Information-based Nonparametric Exploration — 
# 
# <li>statsmodels https://github.com/statsmodels/statsmodels/
# 

import pandas as pd
import numpy as np
import seaborn
from sklearn import linear_model
from sklearn.metrics import r2_score
from matplotlib import pyplot as plt
# huge images can't be displayed on a screen, so directly use Agg
import matplotlib
matplotlib.use("Agg")
from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor
import statsmodels.api as sm
#import statsmodels.formula.api as smf
from scipy import stats
from DataSciency import DataSciency
import os, sys

get_ipython().magic('matplotlib inline')
get_ipython().magic('load_ext autoreload')
get_ipython().magic('autoreload 2')

d = DataSciency()

train_sets = []
files = []
max_score = float('-inf') # r2
max_score_params = []
best_name ="no model assessed yet"

for filename in os.listdir("./heavy_tailed_data/"):
    if filename.endswith(".csv"):
        files.append(filename)
        train_sets.append(pd.read_csv('./heavy_tailed_data/'+filename, header= 0))#, names = ['x','y']))
        
train = train_sets[0]                    
col_names = train.columns.values
print col_names
train.head(5)


# data visualization and normality tests
for data, filename in zip(train_sets, files):
    x=data.iloc[:,0]
    y=data.iloc[:,1]
    print len(y), " values for Y train -unique: ", len(set(list(y)))
    print len(x), " values for X test -unique: ", len(set(list(x)))  
    d.test_normality(y)
    d.visualize_normality_for_sample(y)
    d.plot_histogram(y, filename)


# ### FEATURE CORRELATION
# Pearson correlation coefficient ρ -rho- (if it exists, in [-1,1]) does not assume normality, but is only an exhaustive measure of association if the joint distribution is multivariate normal. Given the failed normality test in our case, another option can be using Spearman’s s or Kendall’s tau. Spearman’s rho can be less reliable and interpretable than Kendall’s while Spearman rho is easier to compute).
# 

for filename, data in zip(files,train_sets):
    d.visualize_feature_correlations(data, 'spearman') #'pearson') # assumes Normal distrib. (not in this case)
    d.visualize_feature_correlations(data, 'kendall')


# ### Ordinary Least Squares (OLS), Linear Regressors and Regularizers, and Generalized linear model (GLM) 
# A flexible generalization of ordinary linear regression (OLR) that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. They unify various other statistical models, including linear regression, logistic regression and Poisson regression and are commonly solved using IRLSR (iteratively reweighted least squares method) for maximum likelihood estimation of the model parameters.
# 

def update_best_model_score_and_params(name, model, params, current_score, max_score):
    if current_score > max_score:
        max_score = current_score
        max_score_params = params
        best_model = model
        best_name = name

# Data models 
best_model = None
alpha = 0.5 #[0.5, 1.0]
ridge = Ridge(alpha=alpha, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)

lasso = Lasso(alpha=alpha, fit_intercept=True, max_iter=1000)
l1_ratios = [0.5, 0.7]
EN = ElasticNet(alpha=alpha, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')

# Distribution Families
# Family(link, variance)	The parent class for one-parameter exponential families.
# Binomial([link])	Binomial exponential family distribution.
# Gamma([link])	Gamma exponential family distribution.
# Gaussian([link])	Gaussian exponential family distribution.
# InverseGaussian([link])	InverseGaussian exponential family.
# NegativeBinomial([link, alpha])	Negative Binomial exponential family.
# Poisson([link])	Poisson exponential family.

exp_family_distribs = [sm.families.Gaussian(), sm.families.Gamma()]#, sm.families.InverseGaussian()]#, sm.families.NegativeBinomial(), sm.families.Poisson()] # sm.families.Binomial(),
linear_models = [ridge, lasso, EN]
filenames_log=[]; a_params=[]; b_params = []; r2_logs=[]; model_logs=[];
# Link functions per family (Not all link functions are available for each distribution family). 
# The list of available link functions can be obtained by   sm.families.family.<familyname>.links
for data, filename in zip(train_sets, files):
    max_score = float('-inf') # r2
    max_score_params = []
    best_name ="no model assessed yet"
    
    x, y = d.get_xy_reshaped_for_numpy(data)
    
    # Linear models
    ###### Ordinary Least Squares (OLS) ###############################
    # An intercept is not included by default and should be added by the user. See statsmodels.tools.add_constant.
    X = sm.add_constant(x, prepend=False)
    OLS = sm.OLS(y,X)
    model = OLS.fit()
    params = model.params
    r2 = model.rsquared
    print filename,' OLS Parameters: ', params, ' R2: ', r2
    print(model.summary())
    
    update_best_model_score_and_params("OLS", model, params, r2, max_score)
    filenames_log.append(filename); a_params.append(params[0]); b_params.append(params[1]); r2_logs.append(r2); model_logs.append(model); 
    
    #### 
    d.OLS_nonlinear_curve_but_linear_in_params(x, y)
    
    #### Regularizer regressors ######################################
    for regressor, name in zip([ridge, lasso, EN], ['Ridge','Lasso','ElasticNet']):
        model = d.fit_model(regressor, x, y, file)
        coef = model.coef_.tolist() 
        if isinstance(coef[0], list):
            coef = coef[0][0]
        else:
            coef = coef[0]
        intercept = model.intercept_.tolist()[0] #np.asmatrix(model.intercept_)[0]
        params = [coef, intercept]
        y_pred = model.predict(x)
        r2 = r2_score(y, y_pred) #regressor.rsquared
        print filename,' Regularizer Parameters: ', params, ' R2: ', r2, ' coef and intercept: ',coef, intercept
        update_best_model_score_and_params("Regressor"+name, model, params, r2, max_score)
        filenames_log.append(filename); a_params.append(params[0]); b_params.append(params[1]); r2_logs.append(r2); model_logs.append(model); 
    
    #### GLM #########################################################
    for family in exp_family_distribs:
        glm = sm.GLM(y, x, family=family)
        model = glm.fit()  
        params = model.params
        if len(params)==1:
            params = [params.tolist()[0], 0]
        # LLF: float Value of the loglikelihood function evalued at params.
        print filename," GLM family parameters: ",family,':\n', params, "\nLLF and Pearson Chi2: ",model.llf,' ', model.pearson_chi2,'\n', model.summary()
        update_best_model_score_and_params("GLM_Family_"+str(family), model, params, r2, max_score)
        #filenames_log.append(filename); a_params.append(params[0]); b_params.append(params[1]); r2_logs.append(r2); model_logs.append(model); 
    
    print filename+" Best r2 and params: ",max_score, max_score_params,"\n",best_model, best_name    
    # PLOT MIC
    d.plot_covariance_based_mutual_info_for_categorical_correlations(data, filename)

df = pd.DataFrame({'filename':filenames_log,'a': a_params, 'b':b_params, 'r2': r2_logs,
              'model_name':model_logs})
df = df[['filename','a','b','r2','model_name']]
df.to_csv("./output/regression_results.csv")    


# ###   Robust Linear Regression (RLM) using loss functions: Median Absolute Deviation (MAD) and Huber Loss
# Because we have fat tails in our distribution, and predictions in ridge are strongly influenced by the outliers present in the dataset, we apply he Huber regressor, which is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge. The metrics to use are:
# 
# <li> Huber loss function ('huber' in sklearn.linear_model.SGDRegressor) modifies ‘squared_loss’ to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon.
# <li> bse: An array of the standard errors of the parameters.  The standard errors are taken from the robust covariance matrix specified in the argument to fit.
# <li> chisq : An array of the chi-squared values of the parameter estimates.
# <li> M-estimation in Robust regression stands for "maximum likelihood type"
# <li> Median Absolute Deviation (MAD): sm.robust.scale.stand_mad(x)  or sm.robust.stand_mad(fat_tails) (alternative to Huber's proposal 2 to estimate location and scale: loc, scale = sm.robust.scale.Huber())
# 
# 
# We look mainly at R_squared, coef. of determination, comparable accross different models, once we have optimized the model for the Huber loss function. 
# <li> R^2 is the percentage of the response variable variation that is explained by a linear model:
# 
# R-squared = Explained variation / Total variation
# 
# R-squared is always between 0 and 100%: 0% indicates that the model explains none of the variability of the response data around its mean. 100% indicates that the model explains all the variability of the response data around its mean.
# In general, the higher the R-squared, the better the model fits your data. In practice, R_squared's best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots. 
# NOTE: While R-squared provides an estimate of the strength of the relationship between the model and the response variable, it does not provide a formal hypothesis test for this relationship. We can look at the F-test of overall significance, which determines whether this relationship is statistically significant.
# 

def Huber_regression(x,y, filename):
    """
    Huber regression with scikit-learn
    Since ridge is strongly influenced by the outliers present in the dataset, Huber regressor 
    is less influenced by the outliers since the model uses the linear loss for these. 
    The HuberRegressor is different to Ridge because it applies a linear loss to samples that are classified 
    as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain 
    threshold epsilon. As the parameter epsilon is increased for the Huber regressor, 
    the decision function approaches that of the ridge.
    -Uses Huber loss and returns the fitted model, the coefficients and the r2 coef of determination, best epsilon and alpha
    """
    plt.plot(x, y, 'b.')

    # Fit the huber regressor over a series of epsilon values.
    colors = ['r-', 'b-', 'y-', 'm-']
    best_r2 = float('-inf') # the closer to 1, the better fit the model provides
    alpha = best_alpha = 0.0 # default #for alpha in [0.0001, 0.1, 1.0, 10.0, 0.001, 0.01, 0.0]:
    best_coef = [-1, -1] 
    epsilon_values = [1.35, 1.5, 1.75, 1.9]
    for k, epsilon in enumerate(epsilon_values):
        huber = HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,
                               epsilon=epsilon)
        fitted = huber.fit(x, y)
        coef_ = huber.coef_ * x + huber.intercept_
        plt.plot(x, coef_, colors[k], label=("Huber loss, epsilon %s, alpha %s" % (epsilon, best_alpha)))
        intercept = huber.intercept_  #params.append(intercept)
        r2 = fitted.score(x, y)  
        if r2 > best_r2:
            best_coef = params
            best_intercept = huber.intercept_
            best_r2 = r2
            best_fitted = fitted
            best_epsilon = epsilon
            best_alpha = alpha
    
    # Fit a ridge regressor to compare it to huber regressor.
    ridge = Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True)
    ridge.fit(x, y)
    coef_ridge = ridge.coef_
    coef_ = ridge.coef_ * x + ridge.intercept_
    plt.plot(x, coef_, 'g-', label="ridge regression")

    plt.savefig('./output/HuberRegressorVSRidge_'+filename.replace('.csv','.png'))
    plt.title("Comparison of Huber Regressor vs Ridge: "+filename)
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.legend(loc=1, scatterpoints=1)#numpoints = 1) bbox_to_anchor=(1, 0.5))#loc=1) loc= 'upper left'#plt.legend(loc=0)
    plt.show()
    
    print " Best Huber Regr. fitted model had ", best_coef," coefficients, intercept: ",best_intercept
    print " R2: ", best_r2," alpha: ", best_alpha, " and epsilon= ", best_epsilon
    return best_fitted, best_coef, best_r2, best_epsilon, best_alpha

filenames_log=[]; a_params=[]; b_params = []; r2_logs=[]; model_logs=[]; epsilons = []; alphas = []
for filename, data in zip(files,train_sets):
    x, y = d.get_xy_reshaped_for_numpy(data)
    
    # Huber regression A (no intercepts, and no r2)
    #model, params, r2 = robust_linear_model_Huber_loss_funct(x,y)
    #filenames_log.append(filename); a_params.append(params[0]); b_params.append(params[1]); r2_logs.append(r2); model_logs.append(model); epsilons.append(-1)     
    
    # Huber regression B (my solution)
    model, params, r2, epsilon, alpha = Huber_regression(x,y, filename)
    filenames_log.append(filename); a_params.append(params[0]); b_params.append(params[1]); r2_logs.append(r2); model_logs.append(model); epsilons.append(epsilon); alphas.append(alpha)
    
df = pd.DataFrame({'filename':filenames_log,'a': a_params, 'b':b_params, 'r2': r2_logs,
              'epsilon':epsilons,'alpha': alphas, 'model_name':model_logs})
df = df[['filename','a','b','r2','epsilon','alpha','model_name']]
df.to_csv("./output/robust_regression_results.csv")


# ### Other Future Models to explore
# If all our data would be increasing, and we were not limited to fit a linear model, I would try Isotonic Regression, which finds a non-decreasing approximation of a function while minimizing the MSE (the benefit being that it does not assume any form for the target function such as linearity). 
# 
# Further models to explore more:
# <li> Generalized Least Squares (GLS)
# <li> Ordinary Least Squares (OLS)
# <li> Weighted Least Squares (WLS)
# <li> Generalized Least Squares with autoregressive error terms GLSAR(p)
# 
# <li> ARMA: Auto Regressive Moving Average: http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/tsa_arma_0.html
# <li> Calculate bounded or unbounded linear least squares curve fit using Moore-Penrose pseudoinverse. http://birota.azurewebsites.net/finding-optimal-dimming-levels-to-match-a-desired-lighting-conditions/
# 

